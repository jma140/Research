{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jinlima'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4d3d6c8637b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-4d3d6c8637b9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mcsvWriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_NONNUMERIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mcsvWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Ticker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DocIndex\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"IndexLink\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FilingDate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"NewFilingDate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mcsvOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os,sys,csv,time  #\"time\" helps to break for the url visiting \n",
    "from bs4 import BeautifulSoup   # Need to install this package manually using pip\n",
    "                                # We only import part of the Beautifulsoup4\n",
    "\n",
    "os.chdir('/Users/jinlima/Documents/scripts/') # The location of your file \"LongCompanyList.csv\n",
    "companyListFile = \"CompanyList.csv\" # a csv file with the list of company ticker symbols and names \n",
    "                                    # (the file has a line with headers)\n",
    "IndexLinksFile = \"IndexLinks.csv\" # a csv file (output of the current script) with the list of index links \n",
    "                                  # for each firm (the file has a line with headers)\n",
    "\n",
    "def getIndexLink(tickerCode,FormType):\n",
    "    csvOutput = open(IndexLinksFile,\"a+b\") # \"a+b\" indicates that we are adding lines rather than replacing lines\n",
    "    csvWriter = csv.writer(csvOutput, quoting = csv.QUOTE_NONNUMERIC)\n",
    "    \n",
    "    urlLink = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=\"+tickerCode+\"&type=\"+FormType+\"&dateb=&owner=exclude&count=100\"\n",
    "    pageRequest = urllib2.Request(urlLink)\n",
    "    pageOpen = urllib2.urlopen(pageRequest)\n",
    "    pageRead = pageOpen.read()\n",
    "    \n",
    "    soup = BeautifulSoup(pageRead,\"html.parser\")\n",
    "    \n",
    "    #Check if there is a table to extract / code exists in edgar database\n",
    "    try:\n",
    "        table = soup.find(\"table\", { \"class\" : \"tableFile2\" })\n",
    "    except:\n",
    "        print(\"No tables found or no matching ticker symbol for ticker symbol for\"+tickerCode)\n",
    "        return -1\n",
    "\n",
    "    docIndex = 1\n",
    "    for row in table.findAll(\"tr\"):\n",
    "        cells = row.findAll(\"td\")\n",
    "        if len(cells)==5:\n",
    "            if cells[0].text.strip() == FormType:\n",
    "                link = cells[1].find(\"a\",{\"id\": \"documentsbutton\"})\n",
    "                docLink = \"https://www.sec.gov\"+link['href']\n",
    "                description = cells[2].text.encode('utf8').strip() #strip take care of the space in the beginning and the end\n",
    "                filingDate = cells[3].text.encode('utf8').strip()\n",
    "                newfilingDate = filingDate.replace(\"-\",\"_\")  ### <=== Change date format from 2012-1-1 to 2012_1_1 so it can be used as part of 10-K file names\n",
    "                csvWriter.writerow([tickerCode, docIndex, docLink, description, filingDate,newfilingDate])\n",
    "                docIndex = docIndex + 1\n",
    "    csvOutput.close()\n",
    "\n",
    "\n",
    "def main():  \n",
    "    FormType = \"10-K\"   ### <=== Type your document type here\n",
    "    nbDocPause = 10 ### <=== Type your number of documents to download in one batch\n",
    "    nbSecPause = 0 ### <=== Type your pausing time in seconds between each batch\n",
    "\n",
    "    csvFile = open(companyListFile,\"r\") #<===open and read from a csv file with the list of company \n",
    "                                        # ticker symbols (the file has a line with headers)\n",
    "    csvReader = csv.reader(csvFile,delimiter=\",\")\n",
    "    csvData = list(csvReader)\n",
    "    \n",
    "    csvOutput = open(IndexLinksFile,\"a+b\") #<===open and write to a csv file which will include the list of index links. New rows will be appended.\n",
    "    csvWriter = csv.writer(csvOutput, quoting = csv.QUOTE_NONNUMERIC)\n",
    "    \n",
    "    csvWriter.writerow([\"Ticker\", \"DocIndex\",\"IndexLink\", \"Description\", \"FilingDate\",\"NewFilingDate\"])\n",
    "    csvOutput.close()\n",
    "    \n",
    "    i = 1\n",
    "    for rowData in csvData[1:]:\n",
    "        ticker = rowData[0]\n",
    "        getIndexLink(ticker,FormType)\n",
    "        if i%nbDocPause == 0:\n",
    "            print(i)\n",
    "            print(\"Pause for \"+str(nbSecPause)+\" second .... \")\n",
    "            time.sleep(float(nbSecPause))\n",
    "        i=i+1\n",
    "       \n",
    "    csvFile.close()\n",
    "    print(\"done!\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,sys,csv,urllib2,time\n",
    "from bs4 import BeautifulSoup   #<---- Need to install this package manually using pip\n",
    "\n",
    "os.chdir('/Users/alvinzuyinzheng/Dropbox/PythonWorkshop/scripts/')#<===The location of your file \"LongCompanyList.csv\n",
    "IndexLinksFile = \"IndexLinks.csv\"  #a csv file (output of the 1GetIndexLinks.py script) with the list of index links for each firm (the file has a line with headers)\n",
    "Form10kListFile = \"10kList.csv\"    #a csv file (output of the current script) with the list of 10-K links for each firm (the file has a line with headers)\n",
    "\n",
    "def get10kLink(tickerCode, docIndex, docLink, description, filingDate, newFilingDate, FormType):\n",
    "    csvOutput = open(Form10kListFile,\"a+b\")\n",
    "    csvWriter = csv.writer(csvOutput, quoting = csv.QUOTE_NONNUMERIC)\n",
    "    \n",
    "    pageRequest = urllib2.Request(docLink)\n",
    "    pageOpen = urllib2.urlopen(pageRequest)\n",
    "    pageRead = pageOpen.read()\n",
    "    \n",
    "    soup = BeautifulSoup(pageRead,\"html.parser\")\n",
    "\n",
    "    #Check if there is a table to extract / code exists in edgar database\n",
    "    try:\n",
    "        table = soup.find(\"table\", { \"summary\" : \"Document Format Files\" })\n",
    "    except:\n",
    "        print \"No tables found for link \"+docLink\n",
    "        \n",
    "    for row in table.findAll(\"tr\"):\n",
    "        cells = row.findAll(\"td\")\n",
    "        if len(cells)==5:\n",
    "            if cells[3].text.strip() == FormType:\n",
    "                link = cells[2].find(\"a\")\n",
    "                formLink = \"https://www.sec.gov\"+link['href']\n",
    "                formName = link.text.encode('utf8').strip()\n",
    "                csvWriter.writerow([tickerCode, docIndex, docLink, description, filingDate, newFilingDate, formLink,formName])\n",
    "    csvOutput.close()\n",
    "\n",
    "\n",
    "def main():  \n",
    "    FormType = \"10-K\"   ### <=== Type your document type here\n",
    "    nbDocPause = 10 ### <=== Type your number of documents to download in one batch\n",
    "    nbSecPause = 1 ### <=== Type your pausing time in seconds between each batch\n",
    "\n",
    "    csvFile = open(IndexLinksFile,\"r\") #<===Open and read from a csv file with the list of index links for each firm (the file has a line with headers)\n",
    "    csvReader = csv.reader(csvFile,delimiter=\",\")\n",
    "    csvData = list(csvReader)\n",
    "    \n",
    "    csvOutput = open(Form10kListFile,\"a+b\") #<===open and write to a csv file which will include the list of 10-K links. New rows will be appended.\n",
    "    csvWriter = csv.writer(csvOutput, quoting = csv.QUOTE_NONNUMERIC)\n",
    "    \n",
    "    csvWriter.writerow([\"Ticker\", \"DocIndex\", \"IndexLink\", \"Description\", \"FilingDate\", \"NewFilingDate\", \"Form10KLink\",\"Form10KName\"])\n",
    "    csvOutput.close()\n",
    "    \n",
    "    i = 1\n",
    "    for rowData in csvData[1:]:\n",
    "        Ticker = rowData[0]\n",
    "        DocIndex = rowData[1]\n",
    "        DocLink = rowData[2]\n",
    "        Description = rowData[3]\n",
    "        FileDate = rowData[4]\n",
    "        NewFileDate = rowData[5]\n",
    "        \n",
    "        get10kLink(Ticker,DocIndex,DocLink,Description,FileDate,NewFileDate,FormType)\n",
    "        if i%nbDocPause == 0:\n",
    "            print i\n",
    "            print \"Pause for \"+str(nbSecPause)+\" second .... \"\n",
    "            time.sleep(float(nbSecPause))\n",
    "        i=i+1\n",
    "       \n",
    "    csvFile.close()\n",
    "    print \"done!\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,sys,csv,urllib2,time\n",
    "\n",
    "\n",
    "os.chdir('/Users/alvinzuyinzheng/Dropbox/PythonWorkshop/scripts/')#<===The location of your file \"LongCompanyList.csv\n",
    "htmlSubPath = \"./HTML/\" #<===The subfolder with the 10-K files in HTML format\n",
    "\n",
    "Form10kListFile = \"10kList.csv\" #a csv file (output of the 2Get10kLinks.py script) with the list of 10-K links\n",
    "logFile = \"10kDownloadLog.csv\" #a csv file (output of the current script) with the download history of 10-K forms\n",
    "\n",
    "def dowmload10k(tickerCode, docIndex, docLink, description, filingDate, newFilingDate, formLink,formName):\n",
    "    csvOutput = open(logFile,\"a+b\")\n",
    "    csvWriter = csv.writer(csvOutput, quoting = csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "    try:\n",
    "        pageRequest = urllib2.Request(formLink)\n",
    "        pageOpen = urllib2.urlopen(pageRequest)\n",
    "        pageRead = pageOpen.read()\n",
    "\n",
    "        htmlname = tickerCode+\"_\"+docIndex+\"_\"+newFilingDate+\".htm\"\n",
    "        htmlpath = htmlSubPath+htmlname\n",
    "        htmlfile = open(htmlpath,'wb')\n",
    "        htmlfile.write(pageRead)\n",
    "        htmlfile.close()\n",
    "        csvWriter.writerow([tickerCode, docIndex, docLink, description, filingDate, newFilingDate, formLink,formName, htmlname, \"\"])\n",
    "    except:\n",
    "        csvWriter.writerow([tickerCode, docIndex, docLink, description, filingDate, newFilingDate, formLink,formName, \"\",\"not downloaded\"])\n",
    "\n",
    "    csvOutput.close()\n",
    "\n",
    "def main():\n",
    "    if not os.path.isdir(htmlSubPath):  ### <=== keep all HTML files in this subfolder\n",
    "        os.makedirs(htmlSubPath)\n",
    "    \n",
    "    FormType = \"10-K\"   ### <=== Type your document type here\n",
    "    nbDocPause = 5 ### <=== Type your number of documents to download in one batch\n",
    "    nbSecPause = 1 ### <=== Type your pausing time in seconds between each batch\n",
    "\n",
    "    FormYears = ['2014','2015'] ### <=== Type the years of documents to download here\n",
    "\n",
    "    csvFile = open(Form10kListFile,\"r\") #<===A csv file with the list of company ticker symbols (the file has a line with headers)\n",
    "    csvReader = csv.reader(csvFile,delimiter=\",\")\n",
    "    csvData = list(csvReader)\n",
    "\n",
    "    csvOutput = open(logFile,\"a+b\")\n",
    "    csvWriter = csv.writer(csvOutput, quoting = csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "    csvWriter.writerow([\"Ticker\", \"DocIndex\", \"IndexLink\", \"Description\", \"FilingDate\", \"NewFilingDate\", \"Form10KLink\",\"Form10KName\", \"FileName\",\"Note\"])\n",
    "    csvOutput.close()\n",
    "    \n",
    "    i = 1\n",
    "    for rowData in csvData[1:]:\n",
    "        Ticker = rowData[0]\n",
    "        DocIndex = rowData[1]\n",
    "        IndexLink = rowData[2]\n",
    "        Description = rowData[3]\n",
    "        FilingDate = rowData[4]\n",
    "        NewFilingDate = rowData[5]\n",
    "        FormLink = rowData[6]\n",
    "        FormName = rowData[7]\n",
    "        for year in FormYears:\n",
    "            if year in FilingDate:\n",
    "                if \".htm\" in FormName:\n",
    "                    dowmload10k(Ticker, DocIndex, IndexLink, Description, FilingDate, NewFilingDate, FormLink,FormName)\n",
    "                elif \".txt\" in FormName:\n",
    "                    csvOutput = open(logFile,\"a+b\")\n",
    "                    csvWriter = csv.writer(csvOutput, quoting = csv.QUOTE_NONNUMERIC)\n",
    "                    csvWriter.writerow([Ticker, DocIndex, IndexLink, Description, FilingDate, NewFilingDate, FormLink,FormName, \"\",\"Text format\"])\n",
    "                    csvOutput.close()\n",
    "                else:\n",
    "                    csvOutput = open(logFile,\"a+b\")\n",
    "                    csvWriter = csv.writer(csvOutput, quoting = csv.QUOTE_NONNUMERIC)\n",
    "                    csvWriter.writerow([Ticker, DocIndex, IndexLink, Description, FilingDate, NewFilingDate, FormLink,FormName,\"\", \"No form\"])\n",
    "                    csvOutput.close()\n",
    "            \n",
    "        if i%nbDocPause == 0:\n",
    "            print i\n",
    "            print \"Pause for \"+str(nbSecPause)+\" second .... \"\n",
    "            time.sleep(float(nbSecPause))\n",
    "        i=i+1\n",
    "       \n",
    "    csvFile.close()\n",
    "    print \"done!\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, csv, re, urllib2\n",
    "from bs4 import BeautifulSoup  #<---- Need to install this package manually using pip\n",
    "\n",
    "os.chdir('/Users/alvinzuyinzheng/Dropbox/PythonWorkshop/scripts/')#<===The location of your csv files\n",
    "htmlSubPath = \"./HTML/\" #<===The subfolder with the 10-K files in HTML format\n",
    "txtSubPath = \"./txt/\" #<===The subfolder with the extracted text files\n",
    "\n",
    "DownloadLogFile = \"10kDownloadLog.csv\" #a csv file (output of the 3DownloadHTML.py script) with the download history of 10-K forms\n",
    "ReadLogFile = \"10kReadlog.csv\" #a csv file (output of the current script) showing whether item 1 is successfully extracted from 10-K forms\n",
    "\n",
    "def readHTML(file_name):\n",
    "    input_path = htmlSubPath+file_name\n",
    "    output_path = txtSubPath+file_name.replace(\".htm\",\".txt\")\n",
    "    \n",
    "\n",
    "    input_file = open(input_path,'rb')\n",
    "    page = input_file.read()  #<===Read the HTML file into Python\n",
    "\n",
    "\n",
    "    #Pre-processing the html content by removing extra white space and combining then into one line.\n",
    "    page = page.strip()  #<=== remove white space at the beginning and end\n",
    "    page = page.replace('\\n', ' ') #<===replace the \\n (new line) character with space\n",
    "    page = page.replace('\\r', '') #<===replace the \\r (carriage returns -if you're on windows) with space\n",
    "    page = page.replace('&nbsp;', ' ') #<===replace \"&nbsp;\" (a special character for space in HTML) with space. \n",
    "    page = page.replace('&#160;', ' ') #<===replace \"&#160;\" (a special character for space in HTML) with space.\n",
    "    while '  ' in page:\n",
    "        page = page.replace('  ', ' ') #<===remove extra space\n",
    "\n",
    "    #Using regular expression to extract texts that match a pattern\n",
    "        \n",
    "    #Define pattern for regular expression.\n",
    "        #The following patterns find ITEM 1 and ITEM 1A as diplayed as subtitles\n",
    "        #(.+?) represents everything between the two subtitles\n",
    "    #If you want to extract something else, here is what you should change\n",
    "\n",
    "    #Define a list of potential patterns to find ITEM 1 and ITEM 1A as subtitles   \n",
    "    regexs = ('bold;\\\">\\s*Item 1\\.(.+?)bold;\\\">\\s*Item 1A\\.',   #<===pattern 1: with an attribute bold before the item subtitle\n",
    "              'b>\\s*Item 1\\.(.+?)b>\\s*Item 1A\\.',               #<===pattern 2: with a tag <b> before the item subtitle\n",
    "              'Item 1\\.\\s*<\\/b>(.+?)Item 1A\\.\\s*<\\/b>',         #<===pattern 3: with a tag <\\b> after the item subtitle          \n",
    "              'Item 1\\.\\s*Business\\.\\s*<\\/b(.+?)Item 1A\\.\\s*Risk Factors\\.\\s*<\\/b') #<===pattern 4: with a tag <\\b> after the item+description subtitle \n",
    "\n",
    "    #Now we try to see if a match can be found...\n",
    "    for regex in regexs:\n",
    "        match = re.search (regex, page, flags=re.IGNORECASE)  #<===search for the pattern in HTML using re.search from the re package. Ignore cases.\n",
    "\n",
    "        #If a match exist....\n",
    "        if match:\n",
    "            #Now we have the extracted content still in an HTML format\n",
    "            #We now turn it into a beautiful soup object\n",
    "            #so that we can remove the html tags and only keep the texts\n",
    "            \n",
    "            soup = BeautifulSoup(match.group(1), \"html.parser\") #<=== match.group(1) returns the texts inside the parentheses (.*?) \n",
    "            \n",
    "\n",
    "            #soup.text removes the html tags and only keep the texts\n",
    "            rawText = soup.text.encode('utf8') #<=== you have to change the encoding the unicodes\n",
    "\n",
    "           \n",
    "            #remove space at the beginning and end and the subtitle \"business\" at the beginning\n",
    "            #^ matches the beginning of the text\n",
    "            outText = re.sub(\"^business\\s*\",\"\",rawText.strip(),flags=re.IGNORECASE)\n",
    "            \n",
    "            output_file = open(output_path, \"w\")\n",
    "            output_file.write(outText)  \n",
    "            output_file.close()\n",
    "            \n",
    "            break  #<=== if a match is found, we break the for loop. Otherwise the for loop continues\n",
    "\n",
    "    input_file.close()    \n",
    "\n",
    "    return match\n",
    "\n",
    "def main():\n",
    "\n",
    "    if not os.path.isdir(txtSubPath):  ### <=== keep all texts files in this subfolder\n",
    "        os.makedirs(txtSubPath)\n",
    "        \n",
    "    csvFile = open(DownloadLogFile, \"rb\") #<===A csv file with the list of 10k file names (the file should have no header)\n",
    "    csvReader = csv.reader(csvFile,delimiter=\",\")\n",
    "    csvData = list(csvReader)\n",
    "    \n",
    "    logFile = open(ReadLogFile, \"a+b\") #<===A log file to track which file is successfully extracted\n",
    "    logWriter = csv.writer(logFile, quoting = csv.QUOTE_NONNUMERIC)\n",
    "    logWriter.writerow([\"filename\",\"extracted\"])\n",
    "\n",
    "    i=1\n",
    "    for rowData in csvData[1:]:\n",
    "##        Ticker = rowData[0]\n",
    "##        DocIndex = rowData[1]\n",
    "##        IndexLink = rowData[2]\n",
    "##        Description = rowData[3]\n",
    "##        FilingDate = rowData[4]\n",
    "##        NewFilingDate = rowData[5]\n",
    "##        FormLink = rowData[6]\n",
    "##        FormName = rowData[7]\n",
    "        FileName = rowData[8]\n",
    "        \n",
    "        if \".htm\" in FileName:        \n",
    "            match=readHTML(FileName)\n",
    "            if match:\n",
    "                logWriter.writerow([FileName,\"yes\"])\n",
    "            else:\n",
    "                logWriter.writerow([FileName,\"no\"])\n",
    "        i=i+1\n",
    "        \n",
    "    csvFile.close()\n",
    "\n",
    "    logFile.close()\n",
    "    print \"done!\"\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web data extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,sys,csv,urllib2,time #\"time\" helps to break for the url visiting \n",
    "from bs4 import BeautifulSoup   # Need to install this package manually using pip\n",
    "                                # We only import part of the Beautifulsoup4\n",
    "\n",
    "os.chdir('/Users/alvinzuyinzheng/Dropbox/PythonWorkshop/scripts/')# The location of your file \"LongCompanyList.csv\n",
    "companyListFile = \"CompanyList.csv\" # a csv file with the list of company ticker symbols and names (the file has a line with headers)\n",
    "IndexLinksFile = \"IndexLinks.csv\" # a csv file (output of the current script) with the list of index links for each firm (the file has a line with headers)\n",
    "\n",
    "def getIndexLink(tickerCode,FormType):\n",
    "    csvOutput = open(IndexLinksFile,\"a+b\") # \"a+b\" indicates that we are adding lines rather than replacing lines\n",
    "    csvWriter = csv.writer(csvOutput, quoting = csv.QUOTE_NONNUMERIC)\n",
    "    \n",
    "    urlLink = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=\"+tickerCode+\"&type=\"+FormType+\"&dateb=&owner=exclude&count=100\"\n",
    "    pageRequest = urllib2.Request(urlLink)\n",
    "    pageOpen = urllib2.urlopen(pageRequest)\n",
    "    pageRead = pageOpen.read()\n",
    "    \n",
    "    soup = BeautifulSoup(pageRead,\"html.parser\")\n",
    "    \n",
    "    #Check if there is a table to extract / code exists in edgar database\n",
    "    try:\n",
    "        table = soup.find(\"table\", { \"class\" : \"tableFile2\" })\n",
    "    except:\n",
    "        print \"No tables found or no matching ticker symbol for ticker symbol for\"+tickerCode\n",
    "        return -1\n",
    "\n",
    "    docIndex = 1\n",
    "    for row in table.findAll(\"tr\"):\n",
    "        cells = row.findAll(\"td\")\n",
    "        if len(cells)==5:\n",
    "            if cells[0].text.strip() == FormType:\n",
    "                link = cells[1].find(\"a\",{\"id\": \"documentsbutton\"})\n",
    "                docLink = \"https://www.sec.gov\"+link['href']\n",
    "                description = cells[2].text.encode('utf8').strip() #strip take care of the space in the beginning and the end\n",
    "                filingDate = cells[3].text.encode('utf8').strip()\n",
    "                newfilingDate = filingDate.replace(\"-\",\"_\")  ### <=== Change date format from 2012-1-1 to 2012_1_1 so it can be used as part of 10-K file names\n",
    "                csvWriter.writerow([tickerCode, docIndex, docLink, description, filingDate,newfilingDate])\n",
    "                docIndex = docIndex + 1\n",
    "    csvOutput.close()\n",
    "\n",
    "\n",
    "def main():  \n",
    "    FormType = \"10-K\"   ### <=== Type your document type here\n",
    "    nbDocPause = 10 ### <=== Type your number of documents to download in one batch\n",
    "    nbSecPause = 0 ### <=== Type your pausing time in seconds between each batch\n",
    "\n",
    "    csvFile = open(companyListFile,\"r\") #<===open and read from a csv file with the list of company ticker symbols (the file has a line with headers)\n",
    "    csvReader = csv.reader(csvFile,delimiter=\",\")\n",
    "    csvData = list(csvReader)\n",
    "    \n",
    "    csvOutput = open(IndexLinksFile,\"a+b\") #<===open and write to a csv file which will include the list of index links. New rows will be appended.\n",
    "    csvWriter = csv.writer(csvOutput, quoting = csv.QUOTE_NONNUMERIC)\n",
    "    \n",
    "    csvWriter.writerow([\"Ticker\", \"DocIndex\",\"IndexLink\", \"Description\", \"FilingDate\",\"NewFilingDate\"])\n",
    "    csvOutput.close()\n",
    "    \n",
    "    i = 1\n",
    "    for rowData in csvData[1:]:\n",
    "        ticker = rowData[0]\n",
    "        getIndexLink(ticker,FormType)\n",
    "        if i%nbDocPause == 0:\n",
    "            print i\n",
    "            print \"Pause for \"+str(nbSecPause)+\" second .... \"\n",
    "            time.sleep(float(nbSecPause))\n",
    "        i=i+1\n",
    "       \n",
    "    csvFile.close()\n",
    "    print \"done!\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\tmain()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
